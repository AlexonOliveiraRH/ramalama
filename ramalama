#!/bin/bash

available() {
  command -v "$1" > /dev/null
}

select_container_manager() {
  if available podman; then
    conman_bin="podman"
    return 0
  elif available docker; then
    conman_bin="docker"
    return 0
  fi

  conman_bin="podman"
}

image_available() {
  [ -n "$("${conman[@]}" images -q "$image_name" 2> /dev/null)" ]
}

wcurl() {
  local wcurl_cmd=("curl" "--globoff" "--location" "--proto-default" "https")
  wcurl_cmd+=("--remote-time" "--retry" "10" "--retry-max-time" "10" "$url")
  "${wcurl_cmd[@]}"
}

check_if_in_hf_db() {
  local host="raw.githubusercontent.com"
  local url="https://$host/ericcurtin/ramalama/main/hf-db/$image_name"
  local image_data
  if ! image_available && image_data="$(wcurl)"; then
    local hf_repo
    hf_repo="$(echo "$image_data" | sed -ne "s/^hf-repo\s//pg" | xargs)"
    local model
    model="$(echo "$image_data" | sed -ne "s/^model\s//pg" | xargs)"
    local containerfile="FROM quay.io/ramalama/ramalama:latest
RUN huggingface-cli download $hf_repo $model
RUN ln -s \$(huggingface-cli download $hf_repo $model) /models/$model
LABEL MODEL=/models/$model"
    echo "$containerfile" | "${conman[@]}" build "$vol" -t "$image_name" -
  fi
}

get_model() {
  "${conman[@]}" inspect -f '{{ index .Config.Labels "MODEL"}}' "$image_name"
}

get_dangling_images() {
  "${conman[@]}" images --filter "dangling=true" -q --no-trunc
}

rm_dir() {
  xargs dirname
}

get_model_dir() {
  "${conman_run[@]}" "$image_name" readlink -f "$model" | rm_dir | rm_dir
}

add_dri() {
  if [ -e "/dev/dri" ]; then
    conman_run+=("--device" "/dev/dri")
  fi
}

run_prep() {
  local host_hf="$HOME/.cache/huggingface/"
  mkdir -p "$host_hf"
  vol="-v$host_hf:/root/.cache/huggingface/:z"
  conman_run=("${conman[@]}" "run" "--rm" "-it")
  conman_run+=("--security-opt=label=disable" "-v$HOME:$HOME" "-v/tmp:/tmp")
  conman_run+=("$vol")

  if [ -e "/proc/driver/nvidia/gpus" ] || available nvidia-smi; then
    conman_run+=("--gpus=all" "--device" "nvidia.com/gpu=all")
  elif [ -e "/dev/kfd" ]; then
    for i in /sys/bus/pci/devices/*/mem_info_vram_total; do
      # AMD GPU needs more than 512M VRAM
      if [ "$(< "$i")" -gt "600000000" ]; then
        conman_run+=("--device" "/dev/kfd")
        add_dri
        ngl="true"
        break
      fi
    done
  elif [ "$(uname -m)" = "aarch64" ]; then # Don't do this on x86_64, slow perf
    add_dri
    ngl="true"
  fi
}

rm_cli() {
  local image_name="$1"
  local model
  model="$(get_model)"

  # To be completed, only delete the directory once all associated images, 3b,
  # latest, etc. are removed
  if false; then
    local dir_to_rm
    dir_to_rm=$(get_model_dir)
    "${conman_run[@]}" "$image_name" rm -rf "$dir_to_rm" || true
  fi

  "${conman[@]}" rmi -f "$image_name"
  get_dangling_images | xargs -r "${conman[@]}" rmi -f
}

build_cli() {
  local image_name="$1"

  run_prep
  exec "${conman[@]}" build "$vol" -t "$image_name" .
}

serve_cli() {
  shift 1
  if [ "$#" -lt 1 ]; then
    serve_usage
  fi

  local dryrun="false"
  while [ $# -gt 0 ]; do
    case $1 in
      -d|--dryrun)
        dryrun="true"
        shift 1
        ;;
      -*)
        serve_usage
        ;;
      *)
        local image_name="$1"
        shift # past argument
        ;;
    esac
  done

  run_prep
  check_if_in_hf_db
  local model
  model="$(get_model)"
  conman_run+=("-p" "${RAMALAMA_HOST:-8080}:8080" "$image_name")
  conman_run+=("llama-server" "-m" "$model")
  if $dryrun; then
    echo "${conman_run[@]}"
    return 0
  fi

  exec "${conman_run[@]}"
}

get_llm_store() {
  if [ "$EUID" -eq 0 ]; then
    llm_store="/var/lib/ramalama/storage"
    return 0
  fi

  llm_store="$HOME/.local/share/ramalama/storage"
}

pull_cli() {
  local image_name="$1"

  run_prep
  check_if_in_hf_db
}

serve_usage() {
  echo "Usage:"
  echo "  $(basename "$0") serve MODEL"
  echo
  echo "Aliases:"
  echo "  serve, start"
  echo
  echo "Environment Variables:"
  echo "  RAMALAMA_HOST  The host:port to bind to (default \"0.0.0.0:8080\")"

  return 1
}

run_usage() {
  echo "Usage:"
  echo "  $(basename "$0") run MODEL"

  return 1
}

run_cli() {
  shift 1
  if [ "$#" -lt 1 ]; then
    run_usage
  fi

  local dryrun="false"
  while [ $# -gt 0 ]; do
    case $1 in
      -d|--dryrun)
        dryrun="true"
        shift 1
        ;;
      -*)
        run_usage
        ;;
      *)
        local image_name="$1"
        shift # past argument
        ;;
    esac
  done

  run_prep
  check_if_in_hf_db
  local model
  model="$(get_model)"
  conman_run+=("$image_name" "llama-main" "-m" "$model" "--log-disable")
  conman_run+=("--instruct")
  if $ngl; then
    conman_run+=("-ngl" "999")
  fi

  if $dryrun; then
    echo "${conman_run[@]}"
    return 0
  fi

  exec "${conman_run[@]}"
}

conman_cli() {
  conman=("$1" "--root" "$llm_store")
  shift 1
  exec "${conman[@]}" "$@"
}

usage() {
  echo "Usage:"
  echo "  $(basename "$0") COMMAND"
  echo
  echo "Commands:"
  echo "  run MODEL        Run a model"
  echo "  pull MODEL       Pull a model"
  echo "  serve MODEL      Serve a model"
  echo "  list             List models"
  echo "  rm MODEL         Remove a model"
  echo "  build MODEL      Build a model from a Containerfile"

  return 1
}

mkdirs() {
  local repo_base="$llm_store/models/repo"
  mkdir -p "$repo_base/hf" "$repo_base/ollama" "$repo_base/oci"
}

main() {
  set -eu -o pipefail

  local conman_bin
  select_container_manager
  local llm_store
  get_llm_store
  mkdirs
  local conman=("$conman_bin" "--root" "$llm_store")
  local conman_run
  local vol
  local ngl="false"
  if [ "$#" -lt 1 ]; then
    usage
  fi

  if [ "$1" = "run" ]; then
    run_cli "$@"
  elif [ "$1" = "pull" ]; then
    pull_cli "$2"
  elif [ "$1" = "serve" ] || [ "$1" = "start" ]; then
    serve_cli "$@"
  elif [ "$1" = "podman" ] || [ "$1" = "docker" ]; then
    conman_cli "$@"
  elif [ "$1" = "list" ] || [ "$1" = "ls" ]; then
    exec "${conman[@]}" images -flabel=MODEL
  elif [ "$1" = "rm" ]; then
    rm_cli "$2"
  elif [ "$1" = "build" ]; then
    build_cli "$2"
  else
    usage
  fi
}

main "$@"

