FROM fedora:41

RUN mkdir -p /models
RUN if [ "$(uname -m)" != "aarch64" ]; then \
      dnf install -y rocminfo rocm-opencl rocm-clinfo rocm-hip hipblas \
        hipblas-devel; \
    fi; \
        \
    dnf install -y git jq procps-ng vim clblast-devel vulkan-headers \
      vulkan-loader-devel glslc glslang 'dnf5-command(builddep)' \
      python3-pip && \
    dnf builddep -y llama-cpp && \
    dnf clean all && \
    rm -rf /var/cache/*dnf* && \
    rm -rf /usr/lib64/rocm/gfx8 /usr/lib64/rocm/gfx9 /usr/lib64/rocm/gfx10 \
      /usr/lib64/rocm/gfx11 /usr/lib64/librocsparse.so.1.0 \
      /usr/lib64/librocblas.so.4.1

RUN pip install -U "huggingface_hub[cli]"

ENV LLAMA_CCACHE=0
ENV LLAMA_CURL=1
ENV LLAMA_VULKAN=1
ENV GGML_HIPBLAS=1

RUN git clone -b ramlama https://github.com/ericcurtin/llama.cpp.git && \
    cd llama.cpp && \
    cmake -B build -DLLAMA_CCACHE=0 -DLLAMA_VULKAN=1 -DLLAMA_CURL=1 \
      -DGGML_HIPBLAS=1 && \
    cmake --build build --config Release -j $(nproc) && \
    cd build/bin && \
    for file in *; do \
      if [ -f "$file" ] && [ -x "$file" ]; then \
        echo "$file" && \
        mv "$file" /usr/bin/llama-"$file"; \
      fi; \
    done; \
    cd / && \
    rm -rf llama.cpp
